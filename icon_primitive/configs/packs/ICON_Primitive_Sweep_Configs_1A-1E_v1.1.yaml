spec_version: '1.1'
defaults:
  seeds:
    model_seeds:
    - 0
    - 1
    - 2
    data_seed: 0
    mi_seed: 0
  training:
    epochs: 100
    batch_size: 256
    optimizer: adamw
    lr: 0.0003
    schedule: warmup5_cosine
    weight_decay: 0.01
    grad_clip: 1.0
    early_stopping: false
    dtype_train: fp32
  init:
    weight_init: orthogonal
    bias_init: zeros
    note: Use a single neutral init across all variants to avoid "activation-favored init" fairness attacks.
  kappa_measurement:
    definition: kappa = I(X; Z_tilde)/d_z
    noise_channel:
      type: gaussian_rms_scaled
      sigma: 0.1
      sigma_sanity:
      - 0.05
      - 0.2
    n_eval: 8192
    estimators_supported:
    - infonce
    - mine
    - ksg32
    primary_estimator: infonce
    infonce_saturation_guard:
      rule: flag_if_MI_ge_logB_minus_margin
      margin: 0.1
    ksg:
      preprocess: fixed_orthogonal_projection
      proj_dim: 32
      k: 5
      proj_seed: 0
    sanity_checks:
    - name: permuted_Z_should_give_MI_near_0
      enabled: true
  probes:
    vector_probe:
      stem: frozen_linear
      stem_seed: 123
      stem_out_dim: 256
      width_default: 256
    spatial_probe:
      stem: frozen_conv_or_patch
      stem_seed: 123
      c: 16
      h: 8
      w: 8
      width_default: 16
activation_1A:
  probe: vector_probe
  baseline:
    id: A00
    activation: relu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: 'Baseline: Linear + ReLU (C=1.0)'
  configs:
  - id: A01
    activation: gelu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + GELU
  - id: A02
    activation: silu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + SiLU
  - id: A03
    activation: tanh
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + Tanh
  - id: A04
    activation: sigmoid
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + Sigmoid
  - id: A05
    activation: mish
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + Mish
  - id: A06
    activation: identity
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: Linear + Identity (no activation)
  expected_runs: 21
  success_criteria:
    seed_std_pct_lt: 2.0
    bootstrap_ci_width_pct_lt: 3.0
linear_type_1B:
  probe: spatial_probe
  track_A_shape_matched:
    baseline:
      id: B00
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: dense
      linear_budget: shape_matched
      notes: 'Baseline: Dense (flatten->linear->reshape), shape-matched'
    configs:
    - id: B01
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: conv1x1
      linear_budget: shape_matched
      notes: Conv 1x1, shape-matched
    - id: B02
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: conv3x3
      linear_budget: shape_matched
      notes: Conv 3x3 (pad=1), shape-matched
    - id: B03
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: depthwise
      linear_budget: shape_matched
      notes: Depthwise 3x3 (groups=C), shape-matched
    expected_runs: 12
  track_B_budget_matched_optional:
    enabled: false
    baseline:
      id: B10
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: dense
      linear_budget: params_or_flops_matched
      notes: 'Optional defense track: Dense baseline but width adjusted to match params/FLOPs anchor'
    configs:
    - id: B11
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: conv1x1
      linear_budget: params_or_flops_matched
      notes: Conv 1x1 budget-matched
    - id: B12
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: conv3x3
      linear_budget: params_or_flops_matched
      notes: Conv 3x3 budget-matched (anchor op)
    - id: B13
      activation: relu
      normalization: none
      precision: fp32
      skip: none
      linear_type: depthwise
      linear_budget: params_or_flops_matched
      notes: Depthwise budget-matched
    expected_runs: 12
    notes: 'Optional but recommended to close the main fairness hole: params/FLOPs differences.'
normalization_1C:
  probe: vector_probe
  baseline:
    id: C00
    activation: relu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: 'Baseline: No Normalization (C=1.0)'
  configs:
  - id: C01
    activation: relu
    normalization: layernorm
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: LayerNorm
  - id: C02
    activation: relu
    normalization: rmsnorm
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: RMSNorm
  - id: C03
    activation: relu
    normalization: batchnorm
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: BatchNorm
  - id: C04
    activation: relu
    normalization: groupnorm
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: GroupNorm
  expected_runs: 15
  measurement_points_optional:
  - pre_norm
  - post_norm
  success_criteria:
    seed_std_pct_lt: 2.0
    bootstrap_ci_width_pct_lt: 3.0
precision_1D:
  probe: vector_probe
  mode: ptq
  baseline:
    id: D00
    activation: relu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: 'Baseline: FP32 (train FP32; measure FP32)'
  configs:
  - id: D01
    activation: relu
    normalization: none
    precision: fp16
    skip: none
    linear_type: dense
    width: 256
    notes: FP16 (PTQ from trained FP32)
  - id: D02
    activation: relu
    normalization: none
    precision: bf16
    skip: none
    linear_type: dense
    width: 256
    notes: BF16 (PTQ from trained FP32)
  - id: D03
    activation: relu
    normalization: none
    precision: int8
    skip: none
    linear_type: dense
    width: 256
    notes: INT8 (PTQ; calibration fixed)
  - id: D04
    activation: relu
    normalization: none
    precision: int4
    skip: none
    linear_type: dense
    width: 256
    notes: INT4 (PTQ; calibration fixed)
  quantization_spec:
    weights: symmetric_per_channel
    activations: symmetric_per_tensor
    calibration:
      n_samples: 1024
      indices_seed: 0
      dataset_split: train
    int4:
      clipping_percentile: 99.9
      scheme: uniform_fake_quant
  expected_runs: 15
  success_criteria:
    seed_std_pct_lt: 3.0
skip_1E:
  probe: vector_probe
  baseline:
    id: E00
    activation: relu
    normalization: none
    precision: fp32
    skip: none
    linear_type: dense
    width: 256
    notes: 'Baseline: No Skip (C=1.0)'
  configs:
  - id: E01
    activation: relu
    normalization: none
    precision: fp32
    skip: residual
    linear_type: dense
    width: 256
    skip_scale: variance_preserving
    notes: 'Residual skip with variance-preserving scaling: y=(x+f(x))/sqrt(2)'
  - id: E02
    activation: relu
    normalization: none
    precision: fp32
    skip: dense_concat
    linear_type: dense
    width: 256
    concat_projection: false
    notes: Dense skip via concat([x,f(x)]) (per-dim kappa handles d change); no projection
  extension_dense_concat_projected_optional:
    enabled: false
    configs:
    - id: E03
      activation: relu
      normalization: none
      precision: fp32
      skip: dense_concat
      linear_type: dense
      width: 256
      concat_projection: true
      projection_out_dim: 256
      notes: 'Optional: concat then fixed projection back to d=256 to separate "dim boost" from structure'
    expected_runs: 3
  expected_runs: 9
